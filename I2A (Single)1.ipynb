{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11edd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import argparse\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "from gym import wrappers\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee9bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the environment class\n",
    "class MountainCarWrapper(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        print(\"Original state:\", state)\n",
    "        return np.array(state[0])\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "            next_state, reward, done, _ = self.env.step(action)[0:4]\n",
    "            return np.array(next_state), reward, done  # Return the full next_state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5042833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the environment model\n",
    "class EnvironmentModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(EnvironmentModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim + 1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.state_head = nn.Linear(32, state_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action = torch.tensor(action, dtype=torch.float32).unsqueeze(0).repeat(state.size(0), 1)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        next_state = self.state_head(x)\n",
    "        return next_state\n",
    "\n",
    "class I2A_MountainCar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, rollout_len, hidden_dim=256):\n",
    "        super(I2A_MountainCar, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.rollout_len = rollout_len\n",
    "\n",
    "        # Define the environment model\n",
    "        self.env_model = EnvironmentModel(state_dim, action_dim)\n",
    "\n",
    "        # Define the imagination module (rollout encoders)\n",
    "        self.imagination = nn.Sequential(\n",
    "            nn.Linear(state_dim * rollout_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Define the value head\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action_space):\n",
    "        # Rollout the environment model\n",
    "        imagined_states = []\n",
    "        for _ in range(self.rollout_len):\n",
    "            action = action_space.sample()\n",
    "            state = self.env_model(state, action)\n",
    "            imagined_states.append(state)\n",
    "        imagined_states = torch.cat(imagined_states, dim=-1)\n",
    "\n",
    "        # Encode the imagined states\n",
    "        x = self.imagination(imagined_states)\n",
    "\n",
    "        # Compute the action probabilities\n",
    "        action_probs = self.policy_head(x)\n",
    "\n",
    "        # Compute the state values\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        return action_probs, state_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d4f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining the hyperparameters\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Imagination-Augmented Agents for Deep Reinforcement Learning\")\n",
    "\n",
    "    # Training settings\n",
    "    parser.add_argument(\"--num_episodes\", type=int, default=30, help=\"Number of training episodes\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=20, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--replay_memory_size\", type=int, default=10000, help=\"Size of the replay memory\")\n",
    "    parser.add_argument(\"--rollout_len\", type=int, default=5, help=\"Length of the rollout for imagination\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor for rewards\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate for the optimizer\")\n",
    "\n",
    "    if sys.argv[0].endswith(\"ipykernel_launcher.py\"):\n",
    "        args = parser.parse_args(args=[])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import namedtuple\n",
    "#defining the replayMemory for storing experience trajectories\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Experience(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a7b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original state: (array([-0.5206747,  0.       ], dtype=float32), {})\n",
      "Original state: (array([-0.47647694,  0.        ], dtype=float32), {})\n",
      "Episode: 0, Reward: -39213.0, Timesteps: 39213\n",
      "Original state: (array([-0.43743235,  0.        ], dtype=float32), {})\n",
      "Episode: 1, Reward: -14554.0, Timesteps: 14554\n",
      "Original state: (array([-0.45732078,  0.        ], dtype=float32), {})\n",
      "Episode: 2, Reward: -8613.0, Timesteps: 8613\n",
      "Original state: (array([-0.5042996,  0.       ], dtype=float32), {})\n",
      "Episode: 3, Reward: -23941.0, Timesteps: 23941\n",
      "Original state: (array([-0.4172788,  0.       ], dtype=float32), {})\n",
      "Episode: 4, Reward: -15679.0, Timesteps: 15679\n",
      "Original state: (array([-0.5336763,  0.       ], dtype=float32), {})\n",
      "Episode: 5, Reward: -13511.0, Timesteps: 13511\n",
      "Original state: (array([-0.46712548,  0.        ], dtype=float32), {})\n",
      "Episode: 6, Reward: -18927.0, Timesteps: 18927\n",
      "Original state: (array([-0.4367647,  0.       ], dtype=float32), {})\n",
      "Episode: 7, Reward: -56699.0, Timesteps: 56699\n",
      "Original state: (array([-0.5120137,  0.       ], dtype=float32), {})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     59\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mdone), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Compute the current Q values\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m action_probs, state_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m action_values \u001b[38;5;241m=\u001b[39m action_probs\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute the target Q values\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mI2A_MountainCar.forward\u001b[0;34m(self, state, action_space)\u001b[0m\n\u001b[1;32m     54\u001b[0m imagined_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(imagined_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Encode the imagined states\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimagination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagined_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Compute the action probabilities\u001b[39;00m\n\u001b[1;32m     60\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the namedtuple to store experiences\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# function to select an action using the current policy\n",
    "def select_action(model, state, action_space):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs, _ = model(state, action_space)\n",
    "    m = torch.distributions.Categorical(action_probs)\n",
    "    return m.sample().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main function to train and test the I2A agent\n",
    "def main(args):\n",
    "    # Create the environment\n",
    "    env = MountainCarWrapper()\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    # Instantiate the I2A model and optimizer\n",
    "    model = I2A_MountainCar(env.observation_space.shape[0], env.action_space.n, args.rollout_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Initialize the replay memory\n",
    "    memory = ReplayMemory(args.replay_memory_size)\n",
    "\n",
    "    # Main training loop\n",
    "    for episode in range(args.num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            # Select an action based on the current policy\n",
    "            action = select_action(model, state, env.action_space)\n",
    "\n",
    "\n",
    "            # Execute the action and store the experience\n",
    "            next_state, reward, done= env.step(action)[0:3]\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "\n",
    "            # Update the state and episode reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If enough experiences are collected, perform a training step\n",
    "            if len(memory) >= args.batch_size:\n",
    "                experiences = memory.sample(args.batch_size)\n",
    "                batch = Experience(*zip(*experiences))\n",
    "\n",
    "               \n",
    "                # Prepare the data for training\n",
    "                states = torch.tensor(np.array(batch.state), dtype=torch.float32)\n",
    "                actions = torch.tensor(np.array(batch.action), dtype=torch.long).unsqueeze(1)\n",
    "                rewards = torch.tensor(np.array(batch.reward), dtype=torch.float32).unsqueeze(1)\n",
    "                next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32)\n",
    "                dones = torch.tensor(np.array(batch.done), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "                # Compute the current Q values\n",
    "                action_probs, state_values = model(states, env.action_space)\n",
    "                action_values = action_probs.gather(1, actions)\n",
    "\n",
    "                # Compute the target Q values\n",
    "                _, next_state_values = model(next_states, env.action_space)\n",
    "\n",
    "                target_action_values = rewards + (args.gamma * next_state_values * (1 - dones))\n",
    "\n",
    "                # Compute the loss and perform a training step\n",
    "                loss = (action_values - target_action_values.detach()).pow(2).mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Check if the episode is finished\n",
    "            if done:\n",
    "                print(\"Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                break\n",
    "\n",
    "    # Testing the trained agent\n",
    "    print(\"Testing the trained agent...\")\n",
    "    test_episodes = 10\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            action = select_action(model, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Test Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                test_rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "    print(\"Average test reward: {:.2f}\".format(sum(test_rewards) / test_episodes))\n",
    "    env.close()\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee520605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
