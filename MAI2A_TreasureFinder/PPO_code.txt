#Compute the current Q values for both agents
                
                # #new mthods PPO
                # # Define PPO-specific hyperparameters
                # epsilon_clip = 0.2
                # ppo_epochs = 10
                # batch_size = 60
                
                # # Collect trajectories (assuming you have a function to collect trajectories)
                # #trajectories = Trajectory

                # # Loop through trajectories and compute advantages
                # advantages1 = []
                # advantages2 = []

                # for t in range(len(states1)):
                #     state = states1[t]
                #     action = actions1[t]
                #     reward = rewards1[t]
                #     next_state = next_states1[t]

                #     # Compute Q-values and state-values for both agents
                #     q_value_agent1, _ = model_agent1(state, action)
                #     v_value_agent1, _ = model_agent1(state)

                #     q_value_agent2, _ = model_agent2(state, action)
                #     v_value_agent2, _ = model_agent2(state)

                #     # Calculate advantages
                #     advantage1 = q_value_agent1 - v_value_agent1
                #     advantage2 = q_value_agent2 - v_value_agent2

                #     # Append to advantages lists
                #     advantages1.append(advantage1)
                #     advantages2.append(advantage2)

                # # Convert advantages to tensors
                # advantages1 = torch.cat(advantages1)
                # advantages2 = torch.cat(advantages2)


                # # Initialize optimizers for both agents
                # optimizer_agent1 = optim.Adam(model_agent1.parameters(), lr=hyperparameters_agent1.lr)
                # optimizer_agent2 = optim.Adam(model_agent2.parameters(), lr=hyperparameters_agent2.lr)
                
                # # Initialize old action probabilities
                # action_probs_old_agent1 = torch.zeros(batch_size, 5) 
                # action_probs_old_agent2 = torch.zeros(batch_size, 5) 


                # for _ in range(ppo_epochs):
                #     # Compute the current action probabilities and state values for both agents
                #     action_probs_agent1, state_values_agent1 = model_agent1(states1, actions1)
                #     action_probs_agent2, state_values_agent2 = model_agent2(states2, actions2)

                #     # Compute the ratio of new and old action probabilities
                #     ratio_agent1 = torch.exp(action_probs_agent1 - action_probs_old_agent1)
                #     ratio_agent2 = torch.exp(action_probs_agent2 - action_probs_old_agent2)

                #     # Compute surrogate losses for both agents
                #     surrogate1 = ratio_agent1 * advantages1
                #     surrogate2 = ratio_agent2 * advantages2

                #     # Calculate clipped surrogate losses
                #     clipped_surrogate1 = torch.clamp(ratio_agent1, 1 - epsilon_clip, 1 + epsilon_clip) * advantages1
                #     clipped_surrogate2 = torch.clamp(ratio_agent2, 1 - epsilon_clip, 1 + epsilon_clip) * advantages2

                #     # Choose the minimum of the clipped and unclipped surrogate losses
                #     surrogate_loss_agent1 = -torch.min(surrogate1, clipped_surrogate1).mean()
                #     surrogate_loss_agent2 = -torch.min(surrogate2, clipped_surrogate2).mean()

                #     # Compute the critic loss for both agents
                #     critic_loss_agent1 = F.mse_loss(state_values_agent1, target_state_values1)
                #     critic_loss_agent2 = F.mse_loss(state_values_agent2, target_state_values2)

                #     # Compute the total loss for both agents
                #     loss_agent1 = surrogate_loss_agent1 + critic_loss_agent1
                #     loss_agent2 = surrogate_loss_agent2 + critic_loss_agent2

                #     # Perform optimization steps for both agents
                #     optimizer_agent1.zero_grad()
                #     loss_agent1.backward()
                #     optimizer_agent1.step()

                #     optimizer_agent2.zero_grad()
                #     loss_agent2.backward()
                #     optimizer_agent2.step()

                # # Update the old action probabilities for the next iteration
                # action_probs_old_agent1 = action_probs_agent1.detach()
                # action_probs_old_agent2 = action_probs_agent2.detach()

