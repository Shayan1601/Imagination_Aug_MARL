{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08e0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from random import randint\n",
    "import torch.nn.functional as F\n",
    "from config1 import config\n",
    "from Treasure_Finder_gymformat import TreasureFinderEnv\n",
    "from DistillPolicy import DistillPolicyAgent\n",
    "from config1 import hyperparameters_agent1, hyperparameters_agent2\n",
    "\n",
    "\n",
    "class EnvironmentModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(EnvironmentModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.flattened_state_dim = state_dim[0] * state_dim[1] * state_dim[2]\n",
    "        # actions is passed as a one-hot matrix instead of a one-hot vector\n",
    "        self.action_dim = (action_dim, *state_dim[1:])  \n",
    "        self.total_input_dim = (state_dim[0] + action_dim, *state_dim[1:])\n",
    "        # Load hyperparameters from the config file\n",
    "        conv1_out_channels = config[\"conv1_out_channels\"]\n",
    "        conv1_filter_size = config[\"conv1_filter_size\"]\n",
    "        conv1_stride = config[\"conv1_stride\"]\n",
    "        conv2_out_channels = config[\"conv2_out_channels\"]\n",
    "        conv2_filter_size = config[\"conv2_filter_size\"]\n",
    "        conv2_stride = config[\"conv2_stride\"]\n",
    "        fc1_out_dim = config[\"fc1_out_dim\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.total_input_dim[0], conv1_out_channels, conv1_filter_size, conv1_stride)\n",
    "        self.conv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, conv2_filter_size, conv2_stride)\n",
    "        self.fc1_dim = self.compute_fc1_dim()\n",
    "        self.fc1 = nn.Linear(self.fc1_dim, fc1_out_dim)\n",
    "        self.state_head = nn.Linear(fc1_out_dim, self.flattened_state_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action_one_hot_matrix = self.one_hot(action)\n",
    "        # Cat on second dimension (1st) since the first (0th) is the batch\n",
    "        state_action = torch.cat([state, action_one_hot_matrix], dim=1)\n",
    "        state_action = state_action.to(torch.float32)\n",
    "        x = F.relu(self.conv1(state_action))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        next_state = self.state_head(x)\n",
    "        next_state = next_state.view(next_state.size(0), *self.state_dim)\n",
    "        return next_state\n",
    "    \n",
    "    def compute_fc1_dim(self):\n",
    "        \"\"\"\n",
    "        Computes the input dimension of the first fully connected layer.\n",
    "        \"\"\"\n",
    "        x = torch.zeros(1, *self.total_input_dim)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def one_hot(self, action):\n",
    "        action_int = int(action)  # Convert the string to an integer\n",
    "        action_one_hot_matrix = torch.zeros((1, *self.action_dim))\n",
    "        action_one_hot_matrix[0, action_int] = 1\n",
    "        return action_one_hot_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e4612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "state_dim = (7, 7, 3)\n",
    "action_dim = 5\n",
    "\n",
    "state_dim = (state_dim[-1], *state_dim[:2])\n",
    "#state_dim = state_dim[0] * state_dim[1] * state_dim[2] #flattened\n",
    "\n",
    "\n",
    "env = TreasureFinderEnv(7)\n",
    "#create distilled policy\n",
    "#distilled =DistillPolicyAgent(state_dim, action_dim)\n",
    "#create the environment model\n",
    "env_model = EnvironmentModel(state_dim, action_dim)\n",
    "\n",
    "\n",
    "\n",
    "#define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(env_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Defining the replay memory for both agents\n",
    "Experience = namedtuple('Experience', ('state', 'action_list', 'next_state', 'done'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action_list, next_state, done):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Experience(state=state, action_list=action_list, next_state=next_state, done=done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        # TODO: take chunks of trajectories instead of single experiences\n",
    "        states, actions, next_states, dones = zip(*[self.memory[i] for i in batch])\n",
    "        return list(states), list(actions), list(next_states), list(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "# Creating replay memory for both agents\n",
    "memory_agent1 = ReplayMemory(hyperparameters_agent1.replay_memory_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8ed83c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 86\u001b[0m\n\u001b[1;32m     80\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(dones)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#print(\"state shape befoe feeding to model:\", states1.shape, states2.shape) --> ([60,3,7,7])\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m predicted_state \u001b[38;5;241m=\u001b[39m \u001b[43menv_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_state, next_states1)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mEnvironmentModel.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m---> 40\u001b[0m     action_one_hot_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Cat on second dimension (1st) since the first (0th) is the batch\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     state_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action_one_hot_matrix], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mEnvironmentModel.one_hot\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 62\u001b[0m     action_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert the string to an integer\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     action_one_hot_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim))\n\u001b[1;32m     64\u001b[0m     action_one_hot_matrix[\u001b[38;5;241m0\u001b[39m, action_int] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    " # Main training loop\n",
    "\n",
    "\n",
    "input_size = (7, 7, 3)  # Update with the appropriate state size attribute\n",
    "output_size = 5\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "\n",
    "\n",
    "for episode in range(hyperparameters_agent1.num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.swapaxes(state, 2, 0)\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    #print(\"reset environment:\", state.shape) #(1,3,7,7)\n",
    "    dones = False\n",
    "    #episode_reward_agent1 = 0\n",
    "    #episode_reward_agent2 = 0\n",
    "    max_time_steps = 100  # Maximum number of time steps\n",
    "\n",
    "    for t in count():\n",
    "        # Select actions based on the current policies for both agents\n",
    "\n",
    "        action1 = (randint(0,5),) \n",
    "        action2 = (randint(0,5),)\n",
    "        action3 = str(action1 * 10 + action2).zfill(2)  #concat into int\n",
    "\n",
    "        # Execute the actions and store the experiences for both agents\n",
    "        action_list= action3\n",
    "        next_state1,reward, done = env.step(action_list)\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(\"state shape right after GET_GlobaL\",next_state1.shape) --> (7,7,3)\n",
    "        next_state1 = np.transpose(next_state1, (2, 0, 1))  # Transpose dimensions to (channels, height, width)\n",
    "        next_state1 = np.expand_dims(next_state1, axis=0)  # Add an extra dimension for the batch\n",
    "        #next_state2 = next_state1\n",
    "        #next_state2 = np.transpose(next_state2, (2, 0, 1))  # Transpose dimensions to (channels, height, width)\n",
    "        #next_state2 = np.expand_dims(next_state2, axis=0)\n",
    "        #print(\"next state shape right after GET_GlobaL\",next_state1.shape) (1,3,7,7)\n",
    "        \n",
    "        \n",
    "\n",
    "        if state.shape != (1,3,7,7) and next_state1 != (1,3,7,7):\n",
    "            print(\"ERROR\")\n",
    "            break\n",
    "        memory_agent1.push(state, action_list, next_state1, done)\n",
    "        #memory_agent2.push(state, action_agent2, reward, next_state2, done)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN ENV Model #\n",
    "        ###############\n",
    "\n",
    "        if len(memory_agent1) >= hyperparameters_agent1.batch_size:\n",
    "\n",
    "            states1, actions_list, next_state1, dones = memory_agent1.sample(hyperparameters_agent1.batch_size)\n",
    "            #states2, actions2, rewards2, next_states2, dones = memory_agent2.sample(hyperparameters_agent2.batch_size)\n",
    "            #print(\"state shape right after SAMPLING\",states1) #-> 60\n",
    "            #print(states1.shape)\n",
    "            #Trajectory = [states1, actions_list, next_states1]\n",
    "\n",
    "            \n",
    "            for i, state in enumerate(states1):\n",
    "                #print(state.shape)\n",
    "                states1[i] = torch.Tensor(state)  # Convert to a PyTorch tensor\n",
    "            states1 = torch.cat(states1)\n",
    "\n",
    "\n",
    "            next_state_tensor1 = []\n",
    "            for next_state in next_state1:\n",
    "                next_state_tensor1.append(torch.Tensor(next_state))\n",
    "\n",
    "            next_states1 = torch.cat(next_state_tensor1)\n",
    "\n",
    "\n",
    "            #actions_list = torch.LongTensor(actions_list)\n",
    "            #actions2 = torch.LongTensor(actions2)\n",
    "            #rewards1 = torch.Tensor(rewards1)\n",
    "            #rewards2 = torch.Tensor(rewards2)\n",
    "            dones = torch.Tensor(dones)\n",
    "\n",
    "            #print(\"state shape befoe feeding to model:\", states1.shape, states2.shape) --> ([60,3,7,7])\n",
    "            \n",
    "            # Forward pass\n",
    "            \n",
    "            predicted_state = env_model(states1, actions_list)\n",
    "\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predicted_state, next_states1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the states and episode rewards for both agents\n",
    "        state = next_state1\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "        if done or t >= max_time_steps:\n",
    "            break\n",
    "    # Print loss for monitoring\n",
    "    print(f\"Episode {episode + 1}/{hyperparameters_agent1.num_episodes}, Loss: {loss.item}\")\n",
    "        \n",
    "\n",
    "        \n",
    "# Save the trained environment model\n",
    "torch.save(env_model.state_dict(), \"env_model.pth\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8af26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
