{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11edd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym import wrappers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee9bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the environment class\n",
    "class MountainCarWrapper(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            return np.array(state[0])\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self.env.step(action)[0:4]  \n",
    "        return np.array(next_state), reward, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the imagination core + rollout enconder \n",
    "\n",
    "class I2A_MountainCar(nn.Module):\n",
    "    def __init__(self, num_actions, rollout_len):\n",
    "        super(I2A_MountainCar, self).__init__()\n",
    "\n",
    "        # Define the imagination module\n",
    "        self.imagination = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3 * rollout_len)\n",
    "        )\n",
    "\n",
    "        # Define the encoder module\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Define the core module\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Linear(32 + 3 * rollout_len, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Define the policy and value heads\n",
    "        self.policy_head = nn.Linear(64, num_actions)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, rollout_len):\n",
    "        # Imagination module\n",
    "        imagined_rollout = self.imagination(state)\n",
    "\n",
    "        # Encoder module\n",
    "        encoded_state = self.encoder(state)\n",
    "\n",
    "        # Concatenate the imagined rollout and encoded state\n",
    "        x = torch.cat([imagined_rollout, encoded_state], dim=1)\n",
    "\n",
    "        # Core module\n",
    "        x = self.core(x)\n",
    "\n",
    "        # Policy and value heads\n",
    "        action_probs = F.softmax(self.policy_head(x), dim=1)\n",
    "        state_value = self.value_head(x)\n",
    "\n",
    "        return action_probs, state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5042833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd approach\n",
    "class EnvironmentModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(EnvironmentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3_state = nn.Linear(hidden_dim, state_dim)\n",
    "        self.fc3_reward = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        next_state = self.fc3_state(x)\n",
    "        reward = self.fc3_reward(x)\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "class I2A_MountainCar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, rollout_len, hidden_dim=32):\n",
    "        super(I2A_MountainCar, self).__init__()\n",
    "\n",
    "        self.rollout_len = rollout_len\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Define the environment model\n",
    "        self.env_model = EnvironmentModel(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "        # Define the imagination module (rollout encoders)\n",
    "        self.imagination = nn.Sequential(\n",
    "            nn.Linear(state_dim * rollout_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the model-free encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the core module\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the policy and value heads\n",
    "        self.policy_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action_space):\n",
    "        # Generate imagined rollouts\n",
    "        imagined_states = []\n",
    "        for _ in range(self.rollout_len):\n",
    "            action = action_space.sample()\n",
    "            state, _ = self.env_model(state, action)\n",
    "            imagined_states.append(state)\n",
    "        imagined_states = torch.cat(imagined_states, dim=-1)\n",
    "\n",
    "        # Imagination module (rollout encoders)\n",
    "        imagined_rollout = self.imagination(imagined_states)\n",
    "\n",
    "        # Encoder module\n",
    "        encoded_state = self.encoder(state)\n",
    "\n",
    "        # Concatenate the imagined rollout and encoded state\n",
    "        x = torch.cat([imagined_rollout, encoded_state], dim=-1)\n",
    "\n",
    "        # Core module\n",
    "        x = self.core(x)\n",
    "\n",
    "        # Policy and value heads\n",
    "        action_probs = F.softmax(self.policy_head(x), dim=-1)\n",
    "        state_value = self.value_head(x)\n",
    "\n",
    "        return action_probs, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d4f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Imagination-Augmented Agents for Deep Reinforcement Learning\")\n",
    "\n",
    "    # Training settings\n",
    "    parser.add_argument(\"--num_episodes\", type=int, default=30, help=\"Number of training episodes\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=20, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--replay_memory_size\", type=int, default=10000, help=\"Size of the replay memory\")\n",
    "    parser.add_argument(\"--rollout_len\", type=int, default=5, help=\"Length of the rollout for imagination\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor for rewards\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate for the optimizer\")\n",
    "\n",
    "    if sys.argv[0].endswith(\"ipykernel_launcher.py\"):\n",
    "        args = parser.parse_args(args=[])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Experience(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a7b3dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     35\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Select an action based on the current policy\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Execute the action and store the experience\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     next_state, reward, done\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(model, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(model, state):\n\u001b[1;32m     13\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     action_probs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(action_probs)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mI2A_MountainCar.forward\u001b[0;34m(self, state, action_space)\u001b[0m\n\u001b[1;32m     59\u001b[0m imagined_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_len):\n\u001b[0;32m---> 61\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m()\n\u001b[1;32m     62\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_model(state, action)\n\u001b[1;32m     63\u001b[0m     imagined_states\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Define the namedtuple to store experiences\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# function to select an action using the current policy\n",
    "def select_action(model, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs, _ = model(state, args.rollout_len)\n",
    "    m = torch.distributions.Categorical(action_probs)\n",
    "    return m.sample().item()\n",
    "\n",
    "# Main function to train and test the I2A agent\n",
    "def main(args):\n",
    "    # Create the environment\n",
    "    env = MountainCarWrapper()\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    # Instantiate the I2A model and optimizer\n",
    "    model = I2A_MountainCar(env.observation_space.shape[0], env.action_space.n, args.rollout_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Initialize the replay memory\n",
    "    memory = ReplayMemory(args.replay_memory_size)\n",
    "\n",
    "    # Main training loop\n",
    "    for episode in range(args.num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            # Select an action based on the current policy\n",
    "            action = select_action(model, state)\n",
    "\n",
    "            # Execute the action and store the experience\n",
    "            next_state, reward, done= env.step(action)[0:3]\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "\n",
    "            # Update the state and episode reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If enough experiences are collected, perform a training step\n",
    "            if len(memory) >= args.batch_size:\n",
    "                experiences = memory.sample(args.batch_size)\n",
    "                batch = Experience(*zip(*experiences))\n",
    "\n",
    "               \n",
    "                # Prepare the data for training\n",
    "                states = torch.tensor(np.array(batch.state), dtype=torch.float32)\n",
    "                actions = torch.tensor(np.array(batch.action), dtype=torch.long).unsqueeze(1)\n",
    "                rewards = torch.tensor(np.array(batch.reward), dtype=torch.float32).unsqueeze(1)\n",
    "                next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32)\n",
    "                dones = torch.tensor(np.array(batch.done), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "                # Compute the current Q values\n",
    "                action_probs, state_values = model(states, args.rollout_len)\n",
    "                action_values = action_probs.gather(1, actions)\n",
    "\n",
    "                # Compute the target Q values\n",
    "                _, next_state_values = model(next_states, args.rollout_len)\n",
    "                target_action_values = rewards + (args.gamma * next_state_values * (1 - dones))\n",
    "\n",
    "                # Compute the loss and perform a training step\n",
    "                loss = (action_values - target_action_values.detach()).pow(2).mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Check if the episode is finished\n",
    "            if done:\n",
    "                print(\"Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                break\n",
    "\n",
    "    # Testing the trained agent\n",
    "    print(\"Testing the trained agent...\")\n",
    "    test_episodes = 10\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            action = select_action(model, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Test Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                test_rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "    print(\"Average test reward: {:.2f}\".format(sum(test_rewards) / test_episodes))\n",
    "    env.close()\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee520605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
