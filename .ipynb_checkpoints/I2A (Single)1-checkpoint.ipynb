{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11edd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n",
      "['/Users/shayan/Desktop/GithubProjects/Imag_aug_MARL/Imagination_Aug_MARL', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cv2', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python310.zip', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/lib-dynload', '', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-macosx-10.9-x86_64.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_maze-0.4-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/gym_notices-0.0.8-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/cloudpickle-2.2.1-py3.10.egg', '/Users/shayan/opt/anaconda3/envs/Env15/lib/python3.10/site-packages/keras_rl-0.4.2-py3.10.egg', '/Users/shayan/slimevolleygym']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym import wrappers\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee9bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the environment class\n",
    "class MountainCarWrapper(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        print(\"Original state:\", state)\n",
    "        return np.array(state[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "            next_state, reward, done, _ = self.env.step(action)[0:4]\n",
    "            return np.array(next_state), reward, done  # Return the full next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5042833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the environment model\n",
    "class EnvironmentModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(EnvironmentModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim + 1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.state_head = nn.Linear(32, state_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        action = torch.tensor(action, dtype=torch.float32).unsqueeze(0).repeat(state.size(0), 1)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        next_state = self.state_head(x)\n",
    "        return next_state\n",
    "\n",
    "class I2A_MountainCar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, rollout_len, hidden_dim=256):\n",
    "        super(I2A_MountainCar, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.rollout_len = rollout_len\n",
    "\n",
    "        # Define the environment model\n",
    "        self.env_model = EnvironmentModel(state_dim, action_dim)\n",
    "\n",
    "        # Define the imagination module (rollout encoders)\n",
    "        self.imagination = nn.Sequential(\n",
    "            nn.Linear(state_dim * rollout_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Define the value head\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action_space):\n",
    "        # Rollout the environment model\n",
    "        imagined_states = []\n",
    "        for _ in range(self.rollout_len):\n",
    "            action = action_space.sample()\n",
    "            state = self.env_model(state, action)\n",
    "            imagined_states.append(state)\n",
    "        imagined_states = torch.cat(imagined_states, dim=-1)\n",
    "\n",
    "        # Encode the imagined states\n",
    "        x = self.imagination(imagined_states)\n",
    "\n",
    "        # Compute the action probabilities\n",
    "        action_probs = self.policy_head(x)\n",
    "\n",
    "        # Compute the state values\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        return action_probs, state_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d4f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "#defining the hyperparameters\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Imagination-Augmented Agents for Deep Reinforcement Learning\")\n",
    "\n",
    "    # Training settings\n",
    "    parser.add_argument(\"--num_episodes\", type=int, default=30, help=\"Number of training episodes\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=20, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--replay_memory_size\", type=int, default=10000, help=\"Size of the replay memory\")\n",
    "    parser.add_argument(\"--rollout_len\", type=int, default=5, help=\"Length of the rollout for imagination\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor for rewards\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate for the optimizer\")\n",
    "\n",
    "    if sys.argv[0].endswith(\"ipykernel_launcher.py\"):\n",
    "        args = parser.parse_args(args=[])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import namedtuple\n",
    "#defining the replayMemory for storing experience trajectories\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Experience(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2a7b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original state: (array([-0.5649926,  0.       ], dtype=float32), {})\n",
      "Original state: (array([-0.49959433,  0.        ], dtype=float32), {})\n",
      "Episode: 0, Reward: -40635.0, Timesteps: 40635\n",
      "Original state: (array([-0.44284338,  0.        ], dtype=float32), {})\n",
      "Episode: 1, Reward: -20165.0, Timesteps: 20165\n",
      "Original state: (array([-0.5096133,  0.       ], dtype=float32), {})\n",
      "Episode: 2, Reward: -83356.0, Timesteps: 83356\n",
      "Original state: (array([-0.42208904,  0.        ], dtype=float32), {})\n",
      "Episode: 3, Reward: -14447.0, Timesteps: 14447\n",
      "Original state: (array([-0.53207344,  0.        ], dtype=float32), {})\n",
      "Episode: 4, Reward: -38118.0, Timesteps: 38118\n",
      "Original state: (array([-0.49127427,  0.        ], dtype=float32), {})\n",
      "Episode: 5, Reward: -55462.0, Timesteps: 55462\n",
      "Original state: (array([-0.579799,  0.      ], dtype=float32), {})\n",
      "Episode: 6, Reward: -38603.0, Timesteps: 38603\n",
      "Original state: (array([-0.46507475,  0.        ], dtype=float32), {})\n",
      "Episode: 7, Reward: -41329.0, Timesteps: 41329\n",
      "Original state: (array([-0.4225286,  0.       ], dtype=float32), {})\n",
      "Episode: 8, Reward: -22381.0, Timesteps: 22381\n",
      "Original state: (array([-0.44896752,  0.        ], dtype=float32), {})\n",
      "Episode: 9, Reward: -71865.0, Timesteps: 71865\n",
      "Original state: (array([-0.44451132,  0.        ], dtype=float32), {})\n",
      "Episode: 10, Reward: -41107.0, Timesteps: 41107\n",
      "Original state: (array([-0.54183954,  0.        ], dtype=float32), {})\n",
      "Episode: 11, Reward: -43196.0, Timesteps: 43196\n",
      "Original state: (array([-0.5407519,  0.       ], dtype=float32), {})\n",
      "Episode: 12, Reward: -8346.0, Timesteps: 8346\n",
      "Original state: (array([-0.45080182,  0.        ], dtype=float32), {})\n",
      "Episode: 13, Reward: -18543.0, Timesteps: 18543\n",
      "Original state: (array([-0.598767,  0.      ], dtype=float32), {})\n",
      "Episode: 14, Reward: -67915.0, Timesteps: 67915\n",
      "Original state: (array([-0.42011192,  0.        ], dtype=float32), {})\n",
      "Episode: 15, Reward: -31196.0, Timesteps: 31196\n",
      "Original state: (array([-0.5434887,  0.       ], dtype=float32), {})\n",
      "Episode: 16, Reward: -55705.0, Timesteps: 55705\n",
      "Original state: (array([-0.40962175,  0.        ], dtype=float32), {})\n",
      "Episode: 17, Reward: -70677.0, Timesteps: 70677\n",
      "Original state: (array([-0.48927745,  0.        ], dtype=float32), {})\n",
      "Episode: 18, Reward: -4293.0, Timesteps: 4293\n",
      "Original state: (array([-0.5674401,  0.       ], dtype=float32), {})\n",
      "Episode: 19, Reward: -59782.0, Timesteps: 59782\n",
      "Original state: (array([-0.55298007,  0.        ], dtype=float32), {})\n",
      "Episode: 20, Reward: -2989.0, Timesteps: 2989\n",
      "Original state: (array([-0.53131336,  0.        ], dtype=float32), {})\n",
      "Episode: 21, Reward: -3071.0, Timesteps: 3071\n",
      "Original state: (array([-0.40660402,  0.        ], dtype=float32), {})\n",
      "Episode: 22, Reward: -12131.0, Timesteps: 12131\n",
      "Original state: (array([-0.4438185,  0.       ], dtype=float32), {})\n",
      "Episode: 23, Reward: -15451.0, Timesteps: 15451\n",
      "Original state: (array([-0.5086637,  0.       ], dtype=float32), {})\n",
      "Episode: 24, Reward: -1352.0, Timesteps: 1352\n",
      "Original state: (array([-0.5604608,  0.       ], dtype=float32), {})\n",
      "Episode: 25, Reward: -10406.0, Timesteps: 10406\n",
      "Original state: (array([-0.45665428,  0.        ], dtype=float32), {})\n",
      "Episode: 26, Reward: -78584.0, Timesteps: 78584\n",
      "Original state: (array([-0.5465861,  0.       ], dtype=float32), {})\n",
      "Episode: 27, Reward: -12703.0, Timesteps: 12703\n",
      "Original state: (array([-0.441031,  0.      ], dtype=float32), {})\n",
      "Episode: 28, Reward: -19913.0, Timesteps: 19913\n",
      "Original state: (array([-0.46821108,  0.        ], dtype=float32), {})\n",
      "Episode: 29, Reward: -62716.0, Timesteps: 62716\n",
      "Testing the trained agent...\n",
      "Original state: (array([-0.576447,  0.      ], dtype=float32), {})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "select_action() missing 1 required positional argument: 'action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    113\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 99\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     96\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m---> 99\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    101\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mTypeError\u001b[0m: select_action() missing 1 required positional argument: 'action_space'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Define the namedtuple to store experiences\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# function to select an action using the current policy\n",
    "def select_action(model, state, action_space):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs, _ = model(state, action_space)\n",
    "    m = torch.distributions.Categorical(action_probs)\n",
    "    return m.sample().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main function to train and test the I2A agent\n",
    "def main(args):\n",
    "    # Create the environment\n",
    "    env = MountainCarWrapper()\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    # Instantiate the I2A model and optimizer\n",
    "    model = I2A_MountainCar(env.observation_space.shape[0], env.action_space.n, args.rollout_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Initialize the replay memory\n",
    "    memory = ReplayMemory(args.replay_memory_size)\n",
    "\n",
    "    # Main training loop\n",
    "    for episode in range(args.num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            # Select an action based on the current policy\n",
    "            action = select_action(model, state, env.action_space)\n",
    "\n",
    "\n",
    "            # Execute the action and store the experience\n",
    "            next_state, reward, done= env.step(action)[0:3]\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "\n",
    "            # Update the state and episode reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If enough experiences are collected, perform a training step\n",
    "            if len(memory) >= args.batch_size:\n",
    "                experiences = memory.sample(args.batch_size)\n",
    "                batch = Experience(*zip(*experiences))\n",
    "\n",
    "               \n",
    "                # Prepare the data for training\n",
    "                states = torch.tensor(np.array(batch.state), dtype=torch.float32)\n",
    "                actions = torch.tensor(np.array(batch.action), dtype=torch.long).unsqueeze(1)\n",
    "                rewards = torch.tensor(np.array(batch.reward), dtype=torch.float32).unsqueeze(1)\n",
    "                next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32)\n",
    "                dones = torch.tensor(np.array(batch.done), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "                # Compute the current Q values\n",
    "                action_probs, state_values = model(states, env.action_space)\n",
    "                action_values = action_probs.gather(1, actions)\n",
    "\n",
    "                # Compute the target Q values\n",
    "                _, next_state_values = model(next_states, env.action_space)\n",
    "\n",
    "                target_action_values = rewards + (args.gamma * next_state_values * (1 - dones))\n",
    "\n",
    "                # Compute the loss and perform a training step\n",
    "                loss = (action_values - target_action_values.detach()).pow(2).mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Check if the episode is finished\n",
    "            if done:\n",
    "                print(\"Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                break\n",
    "\n",
    "    # Testing the trained agent\n",
    "    print(\"Testing the trained agent...\")\n",
    "    test_episodes = 10\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            action = select_action(model, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Test Episode: {}, Reward: {}, Timesteps: {}\".format(episode, episode_reward, t + 1))\n",
    "                test_rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "    print(\"Average test reward: {:.2f}\".format(sum(test_rewards) / test_episodes))\n",
    "    env.close()\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee520605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
